OPENAI_API_KEY=sk-your-key
PORT=3001

# Supabase (for file storage: uploads, stems, MIDI)
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_KEY=your-service-role-key

# ML Functions provider: modal, runpod, or local (default: auto-detect)
FUNCTIONS_PROVIDER=modal
FUNCTIONS_MODAL_URL=https://your-modal-functions-url.modal.run

# RunPod (legacy: use GPU endpoint instead of local Python)
# RUNPOD_ENDPOINT_ID=your-endpoint-id
# RUNPOD_API_KEY=your-runpod-api-key

# Reasoning provider (default: OpenAI). Set runpod_vllm to use RunPod vLLM endpoint.
REASONING_PROVIDER=openai
REASONING_MODEL=gpt-4o-mini

# Provider URLs (optional)
REASONING_FLASH_URL=http://localhost:8888
REASONING_MODAL_URL=

# RunPod reasoning config (when REASONING_PROVIDER=runpod_vllm)
REASONING_RUNPOD_ENDPOINT_ID=your-reasoning-endpoint-id
REASONING_RUNPOD_API_KEY=your-runpod-api-key
# Optional: override full URL (e.g. https://api.runpod.ai/v2/xxx)
REASONING_BASE_URL=

# Legacy PLANNER_* variables are still supported for backward compatibility.